{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdac7ba",
   "metadata": {},
   "source": [
    "# RAG Assistant: Netflix Movies Q&A\n",
    "\n",
    "A production-ready **Retrieval-Augmented Generation (RAG) system** that answers questions about Netflix movies using semantic search and LLMs.\n",
    "\n",
    "## ‚ú® Key Features\n",
    "- **Semantic Search**: 7,655 chunked documents indexed in Chroma vector database\n",
    "- **RAG Pipeline**: Context-aware answers grounded in actual movie data\n",
    "- **REST API**: Flask endpoints (`/ask`, `/health`) for integration\n",
    "- **Clean Architecture**: Separated concerns (notebook for data prep, Python files for production)\n",
    "- **Lightweight**: CPU-optimized embeddings & local LLM inference (no API costs)\n",
    "\n",
    "## üìä Data & Stack\n",
    "- **Data**: 6,020 Netflix movies with plots, genres, directors\n",
    "- **Embeddings**: HuggingFace `all-MiniLM-L6-v2` (384-dimensional)\n",
    "- **Vector DB**: Chroma (persistent, local)\n",
    "- **LLM**: TinyLlama-1.1B-Chat-v1.0 (free, runs locally)\n",
    "- **Framework**: LangChain (LCEL pipe syntax)\n",
    "- **API**: Flask on port 5500\n",
    "\n",
    "## üìÅ Project Structure\n",
    "```\n",
    "‚îú‚îÄ‚îÄ rag_assistant.ipynb      # Data pipeline (Sections 1-9)\n",
    "‚îú‚îÄ‚îÄ app.py                   # Flask API server\n",
    "‚îú‚îÄ‚îÄ rag_pipeline.py          # Reusable RAG initialization\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ documents.csv        # Input: 6,020 Netflix movies\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/         # Output: Chroma persistence\n",
    "‚îî‚îÄ‚îÄ README.md                # Project documentation\n",
    "```\n",
    "\n",
    "## üöÄ Quick Start\n",
    "```bash\n",
    "# Terminal 1: Run the notebook (Sections 1-8) to build vector store\n",
    "jupyter notebook rag_assistant.ipynb\n",
    "\n",
    "# Terminal 2: Start the API\n",
    "source .venv/bin/activate\n",
    "python app.py\n",
    "\n",
    "# Terminal 3: Test the API\n",
    "curl -X POST http://localhost:5500/ask \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"question\": \"What are some comedy movies?\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16722853",
   "metadata": {},
   "source": [
    "## Section 1: Set Up Your Environment & Import Libraries\n",
    "\n",
    "Install dependencies and configure your development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "192a7601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful - ready to load data!\n"
     ]
    }
   ],
   "source": [
    "# 1a: Import core libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 1b: Import LangChain components (updated for current version)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "print(\"‚úì All imports successful - ready to load data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7e725",
   "metadata": {},
   "source": [
    "## Section 2: Load and Prepare Your Documents\n",
    "\n",
    "Load CSV into memory, normalize text, extract metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63dd907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6020, 5)\n",
      "Index(['id', 'source', 'title', 'content', 'category'], dtype='str')\n",
      "Min: 199\n",
      "Max: 1277\n"
     ]
    }
   ],
   "source": [
    "# 2a: Load CSV \n",
    "df = pd.read_csv('/Users/kipronno/rag-assistant/data/documents.csv')\n",
    "\n",
    "# 2b: Inspect the dataframe\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "# print(df.iloc[0])\n",
    "\n",
    "# 2c: Print content length statistics\n",
    "# Documents vary in size. Min/max help you choose good chunk_size later (avoid too small/large).\n",
    "print(f\"Min: {df['content'].str.len().min()}\")\n",
    "print(f\"Max: {df['content'].str.len().max()}\")\n",
    "\n",
    "# 2d: Create normalize_text() function\n",
    "def normalize_text(text):\n",
    "     if pd.isna(text):\n",
    "         return \"\"\n",
    "     return \" \".join(text.split()).replace('\\n', ' ').strip()\n",
    "\n",
    "# 2e: Apply normalization\n",
    "df['content_ready'] = df['content'].apply(normalize_text)\n",
    "# print(\"‚úì Documents loaded and normalized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d270ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An assassin is shot by her ruthless employer, Bill, and other members of their assassination circle ‚Äì but she lives to plot her vengeance. Genre: Action, Crime, Thriller Type: movie Release Year: 2003 Director: Quentin Tarantino Actors: Uma Thurman, Lucy Liu, Vivica A. Fox, Daryl Hannah, David Carradine Uma Thurman Lucy Liu Vivica A Fox Daryl Hannah David Carradine IMDB Rating: 8.2 Available in 67 countries\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text(df.iloc[0]['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8460c2",
   "metadata": {},
   "source": [
    "## Section 3: Split Documents into Chunks\n",
    "\n",
    "Split long documents into manageable chunks with overlap for context preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8d0f8075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7655 chunks\n",
      "Chunk lengths - Min: 55, Max: 500\n",
      "Chunk 0: An assassin is shot by her ruthless employer, Bill, and other members of their assassination circle ‚Äì but she lives to plot her vengeance. Genre: Acti...\n",
      "Chunk 1: Jarhead is a film about a US Marine Anthony Swofford‚Äôs experience in the Gulf War. After putting up with an arduous boot camp, Swofford and his unit a...\n",
      "   doc_id                                  title              source  \\\n",
      "0       1      Kill Bill: Vol. 1 Kill Bill Vol 1  netflix_titles.csv   \n",
      "1       2                                Jarhead  netflix_titles.csv   \n",
      "2       2                                Jarhead  netflix_titles.csv   \n",
      "3       3  Eternal Sunshine of the Spotless Mind  netflix_titles.csv   \n",
      "4       3  Eternal Sunshine of the Spotless Mind  netflix_titles.csv   \n",
      "\n",
      "    category  chunk_idx                                               text  \n",
      "0     Action          0  An assassin is shot by her ruthless employer, ...  \n",
      "1  Biography          0  Jarhead is a film about a US Marine Anthony Sw...  \n",
      "2  Biography          1  . Genre: Biography, Drama, War Type: movie Rel...  \n",
      "3      Drama          0  Joel Barish, heartbroken that his girlfriend u...  \n",
      "4      Drama          1  . Genre: Drama, Romance, Sci-Fi Type: movie Re...  \n"
     ]
    }
   ],
   "source": [
    "# 3a: Initialize RecursiveCharacterTextSplitter with chunk_size=500, overlap=50\n",
    "#  LLMs struggle with long docs (1000+ words). 500-word chunks balance context + focus.\n",
    "# Prevents losing meaning at boundaries. Overlap carries context forward seamlessly.\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "     separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "     chunk_size=500,\n",
    "     chunk_overlap=50\n",
    ")\n",
    "\n",
    "# 3b: Split all documents into chunks\n",
    "#  Creates ~2000 small docs to embed. More chunks = finer-grained semantic search (better precision).\n",
    "# Why metadata fields: Thread info through pipeline so you can tell users WHERE each answer came from.\n",
    "chunks = []\n",
    "for idx, row in df.iterrows():\n",
    "     for i, text in enumerate(splitter.split_text(row['content_ready'])):\n",
    "         chunks.append({\n",
    "             'doc_id': row['id'],\n",
    "             'title': row['title'],\n",
    "             'source': row['source'],\n",
    "             'category': row['category'],\n",
    "             'chunk_idx': i,\n",
    "             'text': text\n",
    "         })\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "\n",
    "# 3c: Show chunk statistics\n",
    "# Why: Verify chunking worked as expected. Odd min/max = adjust chunk_size and re-run.\n",
    "chunk_lengths = [len(c['text']) for c in chunks]\n",
    "print(f\"Chunk lengths - Min: {min(chunk_lengths)}, Max: {max(chunk_lengths)}\")\n",
    "\n",
    "# 3d: Show example chunks\n",
    "# read actual chunks to confirm they're meaningful (not cut mid-word/sentence).\n",
    "example_chunks = [c for c in chunks if 'title' in c][:2]\n",
    "for i, chunk in enumerate(example_chunks):\n",
    "        print(f\"Chunk {i}: {chunk['text'][:150]}...\")\n",
    "\n",
    "# 3e: View as DataFrame\n",
    "chunks_df = pd.DataFrame(chunks)\n",
    "print(chunks_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fe6c8",
   "metadata": {},
   "source": [
    "## Section 4: Create Embeddings for Your Data\n",
    "\n",
    "Generate embeddings for all chunks using HuggingFace (all-MiniLM-L6-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f95cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 7655 chunks\n",
      "Total tokens embedded: 435,082\n"
     ]
    }
   ],
   "source": [
    "# 4a: Initialize HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"batch_size\": 32, \"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# 4b: Batch embed ALL chunks (memory-efficient)\n",
    "\n",
    "all_embeddings = []\n",
    "batch_size = 32\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i+batch_size]\n",
    "    batch_texts = [c['text'] for c in batch]\n",
    "    batch_embeddings = embeddings.embed_documents(batch_texts)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "print(f\"Embedded {len(all_embeddings)} chunks\")\n",
    "\n",
    "# Calculate total tokens\n",
    "total_tokens = sum(len(c['text'].split()) for c in chunks)\n",
    "print(f\"Total tokens embedded: {total_tokens:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb2aff",
   "metadata": {},
   "source": [
    "## Section 5: Build & Query a Local Vector Store\n",
    "\n",
    "Store embeddings in Chroma vector database and test similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd45877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 7655 Document objects\n",
      "‚úì Vector store populated with 7655 documents\n",
      "\n",
      "Query: What are action movies?\n",
      "  - Sakuna: Of Rice and Ruin Sakuna Of Rice and Ruin: . Her new adventure begins! Genre: Action, Adventure, Animation Type: tv Release Year: 2024 Director...\n",
      "  - Toughest Forces on Earth: Three adventurous veterans train alongside some of the world's most elite military units, getting an...\n",
      "\n",
      "Query: Tell me about animated films\n",
      "  - The Grimm Variations: Inspired by the classic Brothers Grimm stories, this anthology features six fairy tales with a dark ...\n",
      "  - Sakuna: Of Rice and Ruin Sakuna Of Rice and Ruin: . Her new adventure begins! Genre: Action, Adventure, Animation Type: tv Release Year: 2024 Director...\n"
     ]
    }
   ],
   "source": [
    "# 5a: Create LangChain Document objects from chunks\n",
    "# Wraps chunks with metadata in a format LangChain understands (standard object type).\n",
    "\n",
    "documents = []\n",
    "for chunk in chunks:\n",
    "    doc = Document(\n",
    "        page_content=chunk['text'],\n",
    "        metadata={\n",
    "            'doc_id': chunk['doc_id'],\n",
    "            'title': chunk['title'],\n",
    "            'source': chunk['source'],\n",
    "            'category': chunk['category'],\n",
    "            'chunk_idx': chunk['chunk_idx']\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"‚úì Created {len(documents)} Document objects\")\n",
    "\n",
    "# 5b: Initialize Chroma vector store\n",
    "# Chroma: Fast similarity search + persistent storage (saves time on re-runs).\n",
    "# persist_directory: Stores vectors on disk so you don't re-embed on next notebook run.\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./data/vectorstore\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 5c: Populate vector store with documents\n",
    "# Process 100 at a time to avoid memory spikes. All docs cumulative.\n",
    "\n",
    "batch_size = 100\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    batch = documents[i:i+batch_size]\n",
    "    batch_ids = [f\"doc_{j}\" for j in range(len(batch))]\n",
    "    vectorstore.add_documents(batch, ids=batch_ids)\n",
    "\n",
    "print(f\"‚úì Vector store populated with {len(documents)} documents\")\n",
    "\n",
    "# 5d: Test similarity search\n",
    "# Confirm vector store populated and working. No results = empty or corrupted store.\n",
    "\n",
    "test_queries = [\"What are action movies?\", \"Tell me about animated films\"]\n",
    "for query in test_queries:\n",
    "    results = vectorstore.similarity_search(query, k=2)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for doc in results:\n",
    "        print(f\"  - {doc.metadata['title']}: {doc.page_content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9d0bd",
   "metadata": {},
   "source": [
    "## Section 6: Create a Retriever\n",
    "\n",
    "Wrap the vector store in a LangChain Retriever for easy integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'action movies'\n",
      "Found 3 similar documents:\n",
      "\n",
      "1. The Hijacking of Flight 601\n",
      "   Preview: After a plane is hijacked, two flight attendants must outwit their assailants amid intense negotiations in the air and o...\n",
      "\n",
      "2. Toughest Forces on Earth\n",
      "   Preview: Three adventurous veterans train alongside some of the world's most elite military units, getting an inside look at thei...\n",
      "\n",
      "3. Toughest Forces on Earth\n",
      "   Preview: Three adventurous veterans train alongside some of the world's most elite military units, getting an inside look at thei...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6a: Retrieve similar documents\n",
    "def find_n_closest(query, retriever, top_k):\n",
    "     \"\"\"Find top-k most similar documents - mirrors Pinecone pattern\"\"\"\n",
    "     docs = retriever.invoke(query)\n",
    "     return docs[:top_k]\n",
    "\n",
    "\n",
    "# 6a: Create retriever (returns top 3 similar docs)\n",
    "retriever = vectorstore.as_retriever(\n",
    "     search_type=\"similarity\",\n",
    "     search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "\n",
    "# 6b: Test retriever with a simple retrieval question\n",
    "# Verify retriever finds semantically similar documents (not analytical questions yet)\n",
    "\n",
    "query = \"action movies\"\n",
    "\n",
    "retrieved_docs = find_n_closest(query, retriever, top_k=3)\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Found {len(retrieved_docs)} similar documents:\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. {doc.metadata['title']}\")\n",
    "    print(f\"   Preview: {doc.page_content[:120]}...\\n\")\n",
    "\n",
    "# 6c: Verify metadata is preserved\n",
    "# Confirm title, source, category carry through the pipeline for later use\n",
    "\n",
    "doc = retrieved_docs[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9858cf",
   "metadata": {},
   "source": [
    "## Section 7: Build the RAG Chain with LLM\n",
    "\n",
    "Create prompt template and connect: Retriever ‚Üí Prompt ‚Üí LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LCEL RAG chain ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 7a: Create PromptTemplate for RAG\n",
    "# This assistant ONLY answers questions about Netflix movies\n",
    "prompt_temp = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a Netflix movie expert. Answer ONLY questions about Netflix movies based on the context below.\n",
    "If the question is not about Netflix movies, say you can only help with Netflix movie questions.\n",
    "Be direct and concise.\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 7b: Format Document objects to clean text\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format Document objects into clean text for LLM\"\"\"\n",
    "    return \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 7c: Initialize TinyLlama-1.1B (1B parameters, lightweight + fast)\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# 7d: Create LCEL RAG chain with proper document formatting\n",
    "def get_retrieved_docs(query):\n",
    "    return retriever.invoke(query)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"retrieved_docs\": RunnableLambda(lambda x: retriever.invoke(x))  # Capture docs\n",
    "    }\n",
    "    | prompt_temp\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# Returns: answer + documents\n",
    "\n",
    "print(\"LCEL RAG chain ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc46608",
   "metadata": {},
   "source": [
    "## Section 8: Test Your RAG System\n",
    "\n",
    "Run manual tests on various question types and inspect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "63342e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "You are a Netflix movie expert. Answer ONLY questions about Netflix movies based on the context below.\n",
      "If the question is not about Netflix movies, say you can only help with Netflix movie questions.\n",
      "Be direct and concise.\n",
      "\n",
      "Context: \n",
      "In this gripping docuseries, legendary reporter George Knapp travels the globe to uncover new evidence about UFOs and investigate their presence on Earth. Genre: Documentary Type: tv Release Year: 2024 Director: Unknown Actors: George Knapp IMDB Rating: 6.5 Available in 128 countries\n",
      "\n",
      "---\n",
      "\n",
      "In this gripping docuseries, legendary reporter George Knapp travels the globe to uncover new evidence about UFOs and investigate their presence on Earth. Genre: Documentary Type: tv Release Year: 2024 Director: Unknown Actors: George Knapp IMDB Rating: 6.5 Available in 128 countries\n",
      "\n",
      "---\n",
      "\n",
      "Eerie encounters, bizarre disappearances, haunting events and more perplexing phenomena are explored in this chilling investigative docuseries. Genre: Crime, Documentary, Mystery Type: tv Release Year: 2024 Director: Unknown Actors: Tijuana Ricks IMDB Rating: 5.4 Available in 129 countries\n",
      "\n",
      "Question: What documentaries are available?\n",
      "\n",
      "Answer:\n",
      "1. \"Gripping Docuseries\"\n",
      "2. \"Legendary Reporter Investigates UFOs\"\n",
      "3. \"Eerie Encounters, Bizarre Disappearances, Haunting Events and More Perplexing Phenomena\"\n",
      "4. \"Chilling Investigative Docuseries\"\n",
      "\n",
      "I hope this helps!\n"
     ]
    }
   ],
   "source": [
    "# 8a: Test the LCEL RAG chain with actual LLM responses\n",
    "query = \"What documentaries are available?\"\n",
    "# 8b: Invoke the chain - retriever ‚Üí format ‚Üí prompt ‚Üí LLM ‚Üí response\n",
    "\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "print(f\"\\nResponse:\\n{result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb02b3",
   "metadata": {},
   "source": [
    "## Section 9: REST API Deployment\n",
    "\n",
    "Deploy the RAG system as a REST API for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask API\n",
    "#\n",
    "# Files created:\n",
    "# - app.py: Flask application with REST endpoints\n",
    "# - rag_pipeline.py: RAG pipeline initialization (embeddings, retriever, LLM chain)\n",
    "#\n",
    "# Architecture (industry standard):\n",
    "# \n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ   HTTP Layer    ‚îÇ  (app.py - Flask routes)\n",
    "# ‚îÇ  /ask, /health  ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "#          ‚Üì\n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ  RAG Logic      ‚îÇ  (rag_pipeline.py - core functionality)\n",
    "# ‚îÇ ask_question()  ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "#          ‚Üì\n",
    "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "# ‚îÇ  Data Layer     ‚îÇ  (Sections 1-8 - retriever, LLM chain)\n",
    "# ‚îÇ  RAG Pipeline   ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "# \n",
    "# Why separate files?\n",
    "# - Cleaner: Notebook focuses on learning, code focuses on production\n",
    "# - Reusable: RAG logic can be imported by multiple services\n",
    "# - Testable: Each layer tested independently\n",
    "# - Scalable: Deploy API and pipeline separately if needed\n",
    "\n",
    "# Running the API\n",
    "# Terminal 1 (run RAG API server):\n",
    "#   $ source .venv/bin/activate\n",
    "#   $ python app.py\n",
    "#   Output: üöÄ Flask API running on http://127.0.0.1:5500\n",
    "#\n",
    "# Terminal 2 (test the API):\n",
    "#   $ curl -X POST http://localhost:5500/ask \\\n",
    "#     -H \"Content-Type: application/json\" \\\n",
    "#     -d '{\"question\": \"What are some comedy movies?\"}'\n",
    "#\n",
    "# Response:\n",
    "#   {\n",
    "#     \"answer\": \"Based on Netflix movies...\",\n",
    "#     \"sources\": [\"Movie Title 1\", \"Movie Title 2\", \"Movie Title 3\"]\n",
    "#   }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
